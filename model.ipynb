{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import keras\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "from skimage import color\n",
    "from skimage import io\n",
    "from skimage import transform\n",
    "import tensorflow as tf\n",
    "# from tensorflow.keras.applications import MobileNetV2\n",
    "# from tensorflow.keras.applications import MobileNet\n",
    "# from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras import backend as k \n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python import debug as tfdebug\n",
    "import os\n",
    "import cv2\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, array([0., 0., 0., ..., 0., 0., 0.])], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = np.load('all/train_images.npy', encoding=\"bytes\")\n",
    "label_arr = np.genfromtxt(\"./all/train_labels.csv\",dtype='str', skip_header=1, delimiter = ',')\n",
    "labels = pd.read_csv('all/train_labels.csv')\n",
    "images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_28(filteredImage):\n",
    "    image, contours, hierarchy = cv2.findContours(filteredImage,cv2.RETR_LIST, \\\n",
    "                                   cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    size=48\n",
    "    if (contours != None):\n",
    "        mx = (0,0,0,0)      # biggest bounding box so far\n",
    "        mx_area = 0\n",
    "    \n",
    "        for cont in contours:\n",
    "            x,y,w,h = cv2.boundingRect(cont)\n",
    "            area = w*h\n",
    "            if area > mx_area:\n",
    "                mx = x,y,w,h\n",
    "                mx_area = area\n",
    "        x,y,w,h = mx\n",
    "        roi=filteredImage[y:y+h,x:x+w]\n",
    "        verti = size- roi.shape[0]\n",
    "        hori = size- roi.shape[1]\n",
    "        top = (size - roi.shape[0])//2\n",
    "        bottom = size - top - roi.shape[0]\n",
    "        left = (size - roi.shape[1])//2\n",
    "        right = size - left - roi.shape[1]\n",
    "        roi_copy = np.zeros((size,size))\n",
    "        for i in range(top, roi.shape[0] + top):\n",
    "            for j in range(left , roi.shape[1] + left):\n",
    "                roi_copy[i][j] = roi[i-top][j-left]     \n",
    "        filteredImage = roi_copy.reshape((size,size))\n",
    "        \n",
    "    else:       \n",
    "        filteredImage = resize(filteredImage, (size,size))\n",
    "    \n",
    "    return filteredImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessImage(image, cutoff=150, maxContours=10):\n",
    "    image = np.uint8(image)\n",
    "    im = np.uint8(image)\n",
    "    red, thresh = cv2.threshold(im, cutoff, 255, 0)\n",
    "    im2, contours, hierarchy= cv2.findContours(thresh, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    mask = np.zeros(im.shape, np.uint8)\n",
    "    largest_contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "\n",
    "    for ind, contour in enumerate(largest_contours[:5]):\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        mask[y:y+h, x:x+w] = 1\n",
    "        \n",
    "    filteredImage = cv2.bitwise_and(thresh, thresh, mask=mask)\n",
    "    filteredImage[filteredImage>1] = 1\n",
    "    filteredImage = filteredImage.reshape((image.shape))\n",
    "    filteredImage = resize_28(filteredImage)\n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    return filteredImage\n",
    "\n",
    "#     digits = ndimage.find_objects(filteredImage)\n",
    "#     max_side = 0\n",
    "#     max_ind = 0\n",
    "#     for i in range(0,len(digits)):\n",
    "#         bound = im[digits[i]]\n",
    "#         if np.max(bound.shape) > max_side:\n",
    "#             max_side = np.max(bound.shape)\n",
    "#             max_ind = i\n",
    "#     print (max_ind)\n",
    "#     bound_rec = im[digits[max_ind]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = preProcessImage(images[5400][1].reshape(100, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 48)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(10000):\n",
    "#     if (True):\n",
    "#         a = preProcessImage(images[i][1].reshape(100, 100))\n",
    "#         #print(a.shape)\n",
    "#         plt.imshow(a)\n",
    "#         plt.figure()\n",
    "#         plt.imshow(images[i][1].reshape(100, 100))\n",
    "        \n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_arr[5400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(a)\n",
    "plt.figure()\n",
    "plt.imshow(images[5400][1].reshape(100, 100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "allData = pd.DataFrame(np.array(list(images[:,1]))).assign(label=labels['Category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(labels['Category'].unique())\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(labels['Category'].unique())\n",
    "valueCounts = labels['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain, xValid = train_test_split(allData, stratify=labels['Category'])\n",
    "trainInds = xTrain.index\n",
    "validInds = xValid.index\n",
    "xTrainRaw = xTrain.drop('label', axis=1).values.reshape((xTrain.shape[0], 100, 100, 1))\n",
    "xValidRaw = xValid.drop('label', axis=1).values.reshape((xValid.shape[0], 100, 100, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "yTrainString = labels.iloc[trainInds, 1].values\n",
    "yValidString = labels.iloc[validInds, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "yTrain = lb.transform(yTrainString)\n",
    "yValid = lb.transform(yValidString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = ThreadPool(multiprocessing.cpu_count())\n",
    "xTrain = pool.map(preProcessImage, [xTrainRaw[i] for i in range(xTrainRaw.shape[0])])\n",
    "xTrain = np.array(xTrain)\n",
    "xValid = pool.map(preProcessImage, [xValidRaw[i] for i in range(xValidRaw.shape[0])])\n",
    "xValid = np.array(xValid)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500, 48, 48)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_data(xTrain, size=48):\n",
    "    Train_x = np.zeros((xTrain.shape[0], size, size, 1))\n",
    "    for i in range(xTrain.shape[0]):\n",
    "        Train_x[i] = xTrain[i].reshape(size, size, 1)\n",
    "    return Train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_x = reshape_data(xTrain)\n",
    "Valid_x = reshape_data(xValid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500, 48, 48, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 1012\n",
    "plt.imshow(Train_x[num][:,:,0])\n",
    "plt.figure()\n",
    "plt.imshow(xTrainRaw[num][:,:,0])\n",
    "yTrainString[num]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelBasic = Sequential()\n",
    "modelBasic.add(Dense(256, activation='relu', input_shape=(36, 36, 1)))\n",
    "modelBasic.add(Dropout(0.25))\n",
    "\n",
    "modelBasic.add(Dense(10, activation='relu'))\n",
    "modelBasic.add(Dropout(0.25))\n",
    "modelBasic.add(Flatten())\n",
    "\n",
    "#modelBasic.add(Activation(tf.nn.softmax))\n",
    "\n",
    "#modelBasic.add(Dense(num_classes, activation='softmax'))\n",
    "modelBasic.compile(loss = keras.losses.categorical_crossentropy,\n",
    "             optimizer = keras.optimizers.Adadelta(),\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = Sequential()\n",
    "model6.add(Conv2D(128, kernel_size=(15, 15), padding='same', activation='relu', input_shape=(36, 36, 1)))\n",
    "model6.add(BatchNormalization(momentum=0.99, epsilon=0.00001))\n",
    "model6.add(MaxPooling2D(pool_size=(10, 10), strides=(7, 7), padding='same'))\n",
    "\n",
    "model6.add(Conv2D(256, kernel_size=(5, 5), padding='same'))\n",
    "model6.add(BatchNormalization(momentum=0.99, epsilon=0.00001))\n",
    "model6.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same'))\n",
    "\n",
    "model6.add(Flatten())\n",
    "\n",
    "model6.add(Dense(384, activation='relu'))\n",
    "model6.add(Dense(192, activation='relu'))\n",
    "\n",
    "\n",
    "model6.add(Dense(num_classes, activation='softmax'))\n",
    "model6.compile(loss = keras.losses.categorical_crossentropy,\n",
    "             optimizer = keras.optimizers.Adadelta(),\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model6\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model6.fit(Train_x, yTrain, batch_size=batch_size,\n",
    "                    epochs=epochs, verbose=1, \n",
    "                    validation_data=(Valid_x, yValid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model7 = Sequential()\n",
    "model7.add(Conv2D(128, kernel_size=(15, 15), padding='same', activation='relu', input_shape=(28, 28, 1)))\n",
    "model7.add(BatchNormalization(momentum=0.99, epsilon=0.00001))\n",
    "model7.add(MaxPooling2D(pool_size=(10, 10), strides=(7, 7), padding='same'))\n",
    "\n",
    "model7.add(Conv2D(256, kernel_size=(5, 5), padding='same'))\n",
    "model7.add(BatchNormalization(momentum=0.99, epsilon=0.00001))\n",
    "model7.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same'))\n",
    "\n",
    "model7.add(Flatten())\n",
    "\n",
    "model7.add(Dense(384, activation='relu'))\n",
    "model7.add(Dense(192, activation='relu'))\n",
    "\n",
    "\n",
    "model7.add(Dense(num_classes, activation='softmax'))\n",
    "model7.compile(loss = keras.losses.categorical_crossentropy,\n",
    "             optimizer = keras.optimizers.SGD(lr=1e-3, momentum=0.0, decay=0.0, nesterov=True),\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model7\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model7.fit(Train_x, yTrain, batch_size=batch_size,\n",
    "                    epochs=epochs, verbose=1, \n",
    "                    validation_data=(Valid_x, yValid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model8 = Sequential()\n",
    "model8.add(Conv2D(128, kernel_size=(15, 15), padding='same', activation='relu', input_shape=(36, 36, 1)))\n",
    "model8.add(BatchNormalization(momentum=0.99, epsilon=0.00001))\n",
    "model8.add(MaxPooling2D(pool_size=(10, 10), strides=(7, 7), padding='same'))\n",
    "\n",
    "model8.add(Conv2D(256, kernel_size=(5, 5), padding='same'))\n",
    "model8.add(BatchNormalization(momentum=0.99, epsilon=0.00001))\n",
    "model8.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same'))\n",
    "\n",
    "model8.add(Flatten())\n",
    "\n",
    "model8.add(Dense(384, activation='relu'))\n",
    "model8.add(Dense(192, activation='relu'))\n",
    "\n",
    "\n",
    "model8.add(Dense(num_classes, activation='softmax'))\n",
    "model8.compile(loss = keras.losses.categorical_crossentropy,\n",
    "             optimizer = keras.optimizers.Nadam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-8, schedule_decay=0.004),\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model8\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model8.fit(Train_x, yTrain, batch_size=batch_size,\n",
    "                    epochs=epochs, verbose=1, \n",
    "                    validation_data=(Valid_x, yValid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 128\n",
    "\n",
    "model9 = Sequential()\n",
    "model9.add(Conv2D(128, kernel_size=(15, 15), padding='same', activation='relu', input_shape=(36, 36, 1)))\n",
    "model9.add(BatchNormalization(momentum=0.99, epsilon=0.00001))\n",
    "model9.add(MaxPooling2D(pool_size=(10, 10), strides=(7, 7), padding='same'))\n",
    "\n",
    "model9.add(Conv2D(256, kernel_size=(5, 5), padding='same'))\n",
    "model9.add(BatchNormalization(momentum=0.99, epsilon=0.00001))\n",
    "model9.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same'))\n",
    "\n",
    "model9.add(Flatten())\n",
    "\n",
    "model9.add(Dense(384, activation='relu'))\n",
    "model9.add(Dense(192, activation='relu'))\n",
    "\n",
    "\n",
    "model9.add(Dense(num_classes, activation='softmax'))\n",
    "model9.compile(loss = keras.losses.categorical_crossentropy,\n",
    "             optimizer = keras.optimizers.Adam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=0.0),\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model9\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model9.fit(Train_x, yTrain, batch_size=64,\n",
    "                    epochs=64, verbose=1, \n",
    "                    validation_data=(Valid_x, yValid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "HEIGHT = 48\n",
    "WIDTH = 48\n",
    "CHANNEL = 1\n",
    "\n",
    "model10 = Sequential()\n",
    "\n",
    "model10.add(Conv2D(50, kernel_size=(5,5),padding='Same', activation='relu', input_shape=(HEIGHT, WIDTH, CHANNEL)))\n",
    "model10.add(Conv2D(50, kernel_size=(5,5),padding='Same', activation='relu'))\n",
    "model10.add(MaxPool2D(pool_size=(2,2)))\n",
    "model10.add(Dropout(0.25))\n",
    "\n",
    "model10.add(Conv2D(100, kernel_size=(3,3),padding='Same', activation='relu'))\n",
    "model10.add(Conv2D(100, kernel_size=(3,3),padding='Same', activation='relu'))\n",
    "model10.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model10.add(Dropout(0.25))\n",
    "\n",
    "model10.add(Flatten())\n",
    "\n",
    "model10.add(Dense(256, activation='relu'))\n",
    "model10.add(Dropout(0.5))\n",
    "\n",
    "model10.add(Dense(num_classes, activation = \"softmax\"))\n",
    "\n",
    "optimizer = optimizers.adam(lr=LEARNING_RATE)\n",
    "model10.compile(optimizer=optimizer , loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/20\n",
      "7500/7500 [==============================] - 148s 20ms/step - loss: 2.5176 - acc: 0.3101 - val_loss: 1.8142 - val_acc: 0.5072\n",
      "Epoch 2/20\n",
      "7500/7500 [==============================] - 150s 20ms/step - loss: 1.7613 - acc: 0.5169 - val_loss: 1.4205 - val_acc: 0.6052\n",
      "Epoch 3/20\n",
      "7500/7500 [==============================] - 147s 20ms/step - loss: 1.4815 - acc: 0.5843 - val_loss: 1.2711 - val_acc: 0.6512\n",
      "Epoch 4/20\n",
      "7500/7500 [==============================] - 146s 19ms/step - loss: 1.2831 - acc: 0.6357 - val_loss: 1.1661 - val_acc: 0.6720\n",
      "Epoch 5/20\n",
      "7500/7500 [==============================] - 141s 19ms/step - loss: 1.1533 - acc: 0.6701 - val_loss: 1.1523 - val_acc: 0.6820\n",
      "Epoch 6/20\n",
      "7500/7500 [==============================] - 143s 19ms/step - loss: 1.0280 - acc: 0.7024 - val_loss: 1.1343 - val_acc: 0.6944\n",
      "Epoch 7/20\n",
      "7500/7500 [==============================] - 144s 19ms/step - loss: 0.9444 - acc: 0.7259 - val_loss: 1.0626 - val_acc: 0.7164\n",
      "Epoch 8/20\n",
      "7500/7500 [==============================] - 148s 20ms/step - loss: 0.8445 - acc: 0.7501 - val_loss: 1.0845 - val_acc: 0.7164\n",
      "Epoch 9/20\n",
      "7500/7500 [==============================] - 144s 19ms/step - loss: 0.7463 - acc: 0.7748 - val_loss: 1.0814 - val_acc: 0.7132\n",
      "Epoch 10/20\n",
      "7500/7500 [==============================] - 143s 19ms/step - loss: 0.7183 - acc: 0.7825 - val_loss: 1.1325 - val_acc: 0.7232\n",
      "Epoch 11/20\n",
      "7500/7500 [==============================] - 142s 19ms/step - loss: 0.6491 - acc: 0.8025 - val_loss: 1.1628 - val_acc: 0.7056\n",
      "Epoch 12/20\n",
      "7500/7500 [==============================] - 144s 19ms/step - loss: 0.5989 - acc: 0.8171 - val_loss: 1.1660 - val_acc: 0.7236\n",
      "Epoch 13/20\n",
      "7500/7500 [==============================] - 147s 20ms/step - loss: 0.5565 - acc: 0.8219 - val_loss: 1.1950 - val_acc: 0.7252\n",
      "Epoch 14/20\n",
      "7500/7500 [==============================] - 145s 19ms/step - loss: 0.5060 - acc: 0.8344 - val_loss: 1.1608 - val_acc: 0.7240\n",
      "Epoch 15/20\n",
      "7500/7500 [==============================] - 149s 20ms/step - loss: 0.4673 - acc: 0.8492 - val_loss: 1.2076 - val_acc: 0.7196\n",
      "Epoch 16/20\n",
      "7500/7500 [==============================] - 147s 20ms/step - loss: 0.4448 - acc: 0.8561 - val_loss: 1.2164 - val_acc: 0.7180\n",
      "Epoch 17/20\n",
      "7500/7500 [==============================] - 147s 20ms/step - loss: 0.4043 - acc: 0.8703 - val_loss: 1.2721 - val_acc: 0.7256\n",
      "Epoch 18/20\n",
      "7500/7500 [==============================] - 148s 20ms/step - loss: 0.3950 - acc: 0.8720 - val_loss: 1.3418 - val_acc: 0.7160\n",
      "Epoch 19/20\n",
      "7500/7500 [==============================] - 143s 19ms/step - loss: 0.3849 - acc: 0.8756 - val_loss: 1.2978 - val_acc: 0.7324\n",
      "Epoch 20/20\n",
      "7500/7500 [==============================] - 141s 19ms/step - loss: 0.3647 - acc: 0.8805 - val_loss: 1.3450 - val_acc: 0.7196\n"
     ]
    }
   ],
   "source": [
    "history = model10.fit(Train_x, yTrain, batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS, verbose=1, \n",
    "                    validation_data=(Valid_x, yValid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#added by Yang\n",
    "TEST_DIR ='all/test_images.npy'\n",
    "TESTSET=np.load(TEST_DIR,encoding='bytes')\n",
    "Testset=TESTSET[:,1]\n",
    "Testset=[Testset[i] for i in range(len(Testset))]\n",
    "Testset=np.array(Testset)\n",
    "Testset=Testset.reshape((Testset.shape[0], 100, 100, 1))\n",
    "pool = ThreadPool(multiprocessing.cpu_count())\n",
    "xTest = pool.map(preProcessImage, [Testset[i] for i in range(Testset.shape[0])])\n",
    "xTest = np.array(xTest)\n",
    "pool.close()\n",
    "pool.join()\n",
    "Test_x = reshape_data(xTest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#added by Yang\n",
    "predictions = model10.predict_classes(Test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#added by Yang\n",
    "TRAIN_LABEL_DIR='all/train_labels.csv'\n",
    "Trainlabel=pd.read_csv(TRAIN_LABEL_DIR)\n",
    "train_labels, train_uniques=pd.factorize(Trainlabel['Category'])\n",
    "dictionary=dict(zip(range(0,32), train_uniques))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=pd.DataFrame(predictions)\n",
    "predictions[0]=[dictionary[item] for item in predictions[0]]\n",
    "predictions=predictions[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0              pear\n",
      "1          sailboat\n",
      "2            pillow\n",
      "3            pillow\n",
      "4             rifle\n",
      "5            pencil\n",
      "6            pillow\n",
      "7         moustache\n",
      "8              pear\n",
      "9             empty\n",
      "10       paintbrush\n",
      "11             pear\n",
      "12           pencil\n",
      "13         squiggle\n",
      "14        pineapple\n",
      "15             nose\n",
      "16           pencil\n",
      "17         squiggle\n",
      "18            mouth\n",
      "19           pencil\n",
      "20           pillow\n",
      "21            spoon\n",
      "22      screwdriver\n",
      "23         squiggle\n",
      "24           peanut\n",
      "25            apple\n",
      "26         scorpion\n",
      "27            panda\n",
      "28            mouth\n",
      "29       rhinoceros\n",
      "           ...     \n",
      "9970    screwdriver\n",
      "9971           sink\n",
      "9972      pineapple\n",
      "9973     skateboard\n",
      "9974          apple\n",
      "9975      moustache\n",
      "9976           sink\n",
      "9977          rifle\n",
      "9978      pineapple\n",
      "9979       scorpion\n",
      "9980       squiggle\n",
      "9981          apple\n",
      "9982          spoon\n",
      "9983          skull\n",
      "9984           pear\n",
      "9985          apple\n",
      "9986           pear\n",
      "9987           pear\n",
      "9988          rifle\n",
      "9989         pillow\n",
      "9990        penguin\n",
      "9991         shovel\n",
      "9992          empty\n",
      "9993      moustache\n",
      "9994      moustache\n",
      "9995          apple\n",
      "9996     paintbrush\n",
      "9997         pencil\n",
      "9998           pear\n",
      "9999          spoon\n",
      "Name: 0, Length: 10000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#added by Yang\n",
    "def save_to_csv(name, result):\n",
    "    import csv\n",
    "    with open(name,'w',newline='') as f:\n",
    "        fieldnames=['Id','Category']\n",
    "        thewriter=csv.DictWriter(f,fieldnames=fieldnames)\n",
    "        thewriter.writeheader()\n",
    "        for i in range(len(result)):\n",
    "            thewriter.writerow({'Id':i,'Category':result[i]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#added by Yang\n",
    "save_to_csv('prediction.csv',predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "HEIGHT = 36\n",
    "WIDTH = 36\n",
    "CHANNEL = 1\n",
    "\n",
    "model12 = Sequential()\n",
    "\n",
    "model12.add(Conv2D(50, kernel_size=(5,5),padding='Same', activation='relu', input_shape=(HEIGHT, WIDTH, CHANNEL)))\n",
    "model12.add(Conv2D(50, kernel_size=(5,5),padding='Same', activation='relu'))\n",
    "model12.add(MaxPool2D(pool_size=(2,2)))\n",
    "model12.add(Dropout(0.25))\n",
    "\n",
    "model12.add(Conv2D(100, kernel_size=(3,3),padding='Same', activation='relu'))\n",
    "model12.add(Conv2D(100, kernel_size=(3,3),padding='Same', activation='relu'))\n",
    "model12.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model12.add(Dropout(0.25))\n",
    "\n",
    "model12.add(Conv2D(200, kernel_size=(3,3),padding='Same', activation='relu'))\n",
    "model12.add(Conv2D(200, kernel_size=(3,3),padding='Same', activation='relu'))\n",
    "model12.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model12.add(Dropout(0.25))\n",
    "\n",
    "model12.add(Flatten())\n",
    "\n",
    "model12.add(Dense(256, activation='relu'))\n",
    "model12.add(Dropout(0.5))\n",
    "\n",
    "model12.add(Dense(num_classes, activation = \"softmax\"))\n",
    "\n",
    "optimizer = optimizers.adam(lr=LEARNING_RATE)\n",
    "model12.compile(optimizer=optimizer , loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model12.fit(Train_x, yTrain, batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS, verbose=1, \n",
    "                    validation_data=(Valid_x, yValid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model12.predict(Valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = 0\n",
    "false = 0\n",
    "false_class = [0]*31\n",
    "for i in range(2500):\n",
    "    if np.argmax(yValid[i]) == np.argmax(predictions[i]):\n",
    "        true += 1\n",
    "    else:\n",
    "        #print(\"True class:\", np.argmax(yValid[i]))\n",
    "        #print(\"False prediction:\", np.argmax(predictions[i]))\n",
    "        false_class[np.argmax(yValid[i])] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_data_2(xTrain, size=36):\n",
    "    Train_x = np.zeros((xTrain.shape[0], size, size))\n",
    "    for i in range(xTrain.shape[0]):\n",
    "        Train_x[i] = xTrain[i].reshape(size, size)\n",
    "    return Train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_x = reshape_data_2(xTrain)\n",
    "Valid_x = reshape_data_2(xValid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_units = 50\n",
    "\n",
    "model11 = Sequential()\n",
    "\n",
    "# Recurrent layers supported: SimpleRNN, LSTM, GRU:\n",
    "model11.add(SimpleRNN(nb_units,\n",
    "                    input_shape=(36, 36)))\n",
    "\n",
    "# To stack multiple RNN layers, all RNN layers except the last one need\n",
    "# to have \"return_sequences=True\".  An example of using two RNN layers:\n",
    "#model.add(SimpleRNN(16,\n",
    "#                    input_shape=(img_rows, img_cols),\n",
    "#                    return_sequences=True))\n",
    "#model.add(SimpleRNN(32))\n",
    "\n",
    "model11.add(Dense(units=num_classes))\n",
    "model11.add(Activation('softmax'))\n",
    "\n",
    "model11.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model11.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_x = reshape_data(xTrain)\n",
    "Valid_x = reshape_data(xValid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 128\n",
    "epochs = 300\n",
    "filter_sizes = [18,18,18]\n",
    "hidden_sz = 128\n",
    "model = Sequential()\n",
    "model.add(Conv2D(18, (5, 5), padding='same', input_shape=(36,36,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "for fs in filter_sizes:\n",
    "    model.add(Conv2D(fs, (1,1)))\n",
    "    model.add(Conv2D(fs, (3,3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(fs, (1,1)))\n",
    "    # model.add(Conv2D(fs, (3,3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(AveragePooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(hidden_sz))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "opt = keras.optimizers.Adam(lr=5e-3)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(Train_x, yTrain, batch_size=batch_sz,\n",
    "                    epochs=epochs, verbose=1, \n",
    "                    validation_data=(Valid_x, yValid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
